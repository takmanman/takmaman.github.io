---
layout: tutorial
title: "A Very Gentle Introduction to Building Convolutional Neural Networks (CNN) in PyTorch"
date: 2019-09-08
---

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3.0.0/es5/tex-mml-chtml.js">
  </script>

In this tutorial, I am going to walk through the necessary steps of building a CNN model in PyTorch. I will also talk about how to calculate the height and width of the output of a convolution layer
in order to avoid size mismatch errors when designing a CNN model.

In the end, I will discuss the dimensions of the kernels and the output of the convolution layers, as well as the number of parameters of a given CNN.

Kernels are often called filters. I will use these terms interchangeably. However, some people consider a filter as a stack of kernels, and therefore kernels are 2d while filters are 3d. In this tutorial, kernels and filters are exactly the same and both are 3-d.

This tutorial is meant to be straight forward and focused only on buiding a CNN model (but not how to train it), and hopefully in the end you will be able to implement any CNN model in PyTorch with ease.

<h2>Building a CNN</h2>

In PyTorch, we build a CNN model by creating a subclass of <span class = "important_term">torch.NN.Module</span>. I am not going to discuss what exactly torch.NN.Module is. For now, I think it is sufficient to say that it is (should be) the base class of all neural network models built in PyTorch.

In the definition of this subclass (let's call it myCNN), we must implement two of its method: <span class = "code_function">\_\_init\_\_()</span> and <span class = "code_function">forward()</span>.

<h3> Example 1 </h3>

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class myCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)
        self.fc = nn.Linear(32*8*8,10)
        
    def forward(self, x):
        x = x.view(-1, 3, 32, 32) #conver to the required dimension(batch size, channels, height, width)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        
        #convert x to a 1-d vector because a fully-connected layer requires a 1-d vector as input
        size = x.size()[1:]  #all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s        
        x = x.view(-1, num_features)
        
        x = self.fc(x)
        
        return x
```
In <span class = "code_function">\_\_init\_\_()</span>, we define the layers. In this example, the network has 2 convolution layers, and one fully-connected layer. 

In <span class = "code_function">forward()</span>, we specify how to stack the layers and how an input image is passed along the network. This is where we specify the activation fucntion for each convolution layer. In Fig. 1,
it depicts a foward pass of an image input to this network.

<figure>
<img src="{{site.url}}/tutorials/images/basic_cnn.png"  style="display: block; margin: auto; width: 60%;"/>
<figcaption>Fig.1 - A forward pass in a basic CNN model</figcaption>
</figure>

If we create an instance from this class, it will give us a CNN model with the specified structure.


```python
cnn = myCNN()
print(cnn)
```

<pre>
<div class = "output">
myCNN(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (fc): Linear(in_features=2048, out_features=10, bias=True)
)
</div>
</pre>

We can then input an image by calling the model itself or using <span class = "code_function">forward()</span>.
 
```python
ip = torch.randn(1, 3, 32, 32)
out = cnn(ip)
print(out)
out2 = cnn.forward(ip)
print(out2)
```
<pre>
<div class = "output">
tensor([[-0.0810,  0.0143,  0.0658,  0.0611,  0.1605,  0.0042, -0.0662,  0.0746,
          0.1831,  0.1606]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[-0.0810,  0.0143,  0.0658,  0.0611,  0.1605,  0.0042, -0.0662,  0.0746,
          0.1831,  0.1606]], grad_fn=&lt;AddmmBackward&gt;)
</div>
</pre>

And this is all you need to do to build a CNN model in PyTorch.

<h2>Kernel Sizes and the Height and Width of the Convolution Layer Output</h2>
<p>
If we instead set the kernel sizes of the convolution layers to 5, and again input an image of size \(32 \times 32\), it is going to give us an error.
</p>
```python
#Replace with these lines in the definition of myCNN
self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=1)
self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=1)
```
<p></p>
```python
ip = torch.randn(1, 3, 32, 32)
out = cnn(ip)
```
<pre>
<div class = "output">
RuntimeError: size mismatch, m1: [1 x 1568], m2: [2048 x 10]
</div>
</pre>

This is because the fully-connected layer at the top of the model is expecting a different input size.

<h3>How to calculate the height and width of a convolution layer output</h3>

<p>
The height and width \((H_{\text{out}}, W_{\text{out}})\) of the output of a convolution layer, given the height and width of the input \((H_{\text{in}}, W_{\text{in}})\), can be calculated using this equation:
</p>

<p>
\[H_{\text{out}} = \lfloor \frac{H_{\text{in}}+2\times \text{padding}[0]-\text{kernel_size}[0]}{\text{stride}[0]}+1\rfloor\]
\[W_{\text{out}} = \lfloor \frac{W_{\text{in}}+2\times \text{padding}[1]-\text{kernel_size}[1]}{\text{stride}[1]}+1\rfloor\]
</p>
<p>
Therefore, for the CNN model in Example 1, if the kernel sizes for both convolution layers are set to 5, then the output of conv2 will be of size \(7 \times 7\).
We can then modify the fully-connected layer accordingly.
</p>
```python
#Replace with this line in the definition of myCNN
self.fc = nn.Linear(32*7*7,10)
```
<p></p>
```python
ip = torch.randn(1, 3, 32, 32)
out = cnn(ip)
print(out)
```
<pre>
<div class = "output">
tensor([[ 0.0236,  0.0631, -0.1651,  0.0212,  0.0478,  0.0933, -0.0332, -0.0417,
          0.0639,  0.0592]], grad_fn=&lt;AddmmBackward&gt;)
</div>
</pre>

In figure 2, it depicts how the output size progresses along a CNN model (when kernel sizes are set to 5).

<figure>
<img src="{{site.url}}/tutorials/images/cnn_output_size.png"  style="display: block; margin: auto; width: 60%;"/>
<figcaption>Fig.2 - How the output size progresses along the model</figcaption>
</figure>

When designing CNN models, it is crucial to know these equations because we often want to try out different kernel sizes.
While small kernels can capture local features, large kernels may be able to provide more contextual information as they cover larger areas in the input image.
<p>
Another reason we would need these equations is that a CNN model always has a default input size. For example, ResNet has a default size of \(224 \times 224\), and InceptionV3 has a default size of \(299 \times 299\).
An input image deviates from the default size is likely to produce a size match error.
</p>
In that case, you will have to scale or crop it. Otherwise, you can also make some adjustment to your model layers in order to fit your image size.

<h2>Kernel Dimensions and Number of Parameters</h2>
<p>
So far, we have only talked about the height and width of a kernel. In fact, it also has a third dimension. In example 1, the kernels of conv1 has the dimension of \(5 \times 5 \times 3\), 
and the kernels of conv2 has the dimension of \(5 \times 5 \times 16\) (after we change the kernel sizes to \(5 \times 5\) in the last section).
</p>
As depicted in figure 3, the kernel convolves with the entire volume of the input to produce one slice of the output.
<figure>
<img src="{{site.url}}/tutorials/images/cnn_kernel_output_dimensions.png"  style="display: block; margin: auto; width: 60%;"/>
<figcaption>Fig.3 - Convolution between a kernel and an input with 3 channels. </figcaption>
</figure>
The depth of the kernel is equal to the number of channels of the input. As the convolution would result in only one slice of the output, if the output has multiple channels, the layer will need multiple kernels. 
This is depicted in fgiure 4 and summarized in the following equations.
<p>
\[\text{kernel depth} = \text{number of channels of the input} \]
</p>
<p>
\[\text{number of the channels of the output} = \text{number of kernels} \]
</p>

<figure>
<img src="{{site.url}}/tutorials/images/cnn_multi_kernel_output_dimensions.png"  style="display: block; margin: auto; width: 60%;"/>
<figcaption>Fig.4 - Convolutions between multiple kernels and an input with 3 channels.</figcaption>
</figure>
<p>
In example 1, conv1 has an input with 3 channels and an output with 16 channels. Therefore it has sixteen \(5 \times 5 \times 3\) kernels, and its number of parameters is 1200. 
The total number of parameters of this CNN model is given by:
</p>
<p>
\[\text{# of parameters of conv1} + \text{# of parameters of conv2} + \text{# of parameters of fc} \]
\[=16 \times (5 \times 5 \times 3) + 32 \times (5 \times 5 \times 16) + (32 \times 7 \times 7 + 1)\times 10\]
\[= 29690\]
</p>
This is the end of this gentle introduction. I hope it will help you designing CNN models using PyTorch.
